{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Keras 3 compatibility issue and install required packages\n",
    "!pip install tf-keras ipykernel plotly\n",
    "print(\"‚úÖ tf-keras, ipykernel, and plotly installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Test if imports work before running the main code\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import transformers\n",
    "    print(\"‚úÖ TensorFlow and Transformers imported successfully\")\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"Transformers version: {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Installing tf-keras to fix compatibility...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'tf-keras'])\n",
    "    print(\"‚úÖ tf-keras installed. Restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGwrUZSxXSuG"
   },
   "source": [
    "# AnyoneAI - Sprint Project 04\n",
    "> Multimodal Product Classification\n",
    "\n",
    "You've been learning a lot about Deep Learning Algorithms, now we you're gonna be asked to put it all together. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680033779007,
     "user": {
      "displayName": "Pablo Pastore",
      "userId": "13868799798828179067"
     },
     "user_tz": 180
    },
    "id": "5bcJoaR6YahS",
    "outputId": "8ffde720-b93c-4b78-e804-f820bf2dbd7e"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "IN_COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify the kernel is working\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(\"‚úÖ Kernel is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the O.S is Mx Mac\n",
    "import platform\n",
    "IN_MAC = platform.system() == \"Darwin\"\n",
    "IN_MAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26895,
     "status": "ok",
     "timestamp": 1680028888362,
     "user": {
      "displayName": "Pablo Pastore",
      "userId": "13868799798828179067"
     },
     "user_tz": 180
    },
    "id": "_9ohQ9PnXeZx",
    "outputId": "640e4dd1-946d-491e-a010-113537e668f7"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1680033781336,
     "user": {
      "displayName": "Pablo Pastore",
      "userId": "13868799798828179067"
     },
     "user_tz": 180
    },
    "id": "MOmDt--lXwUQ",
    "outputId": "107fe30a-da87-4927-b51f-a6532397dce6"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Put here the full path to the folder having your Sprint project code\n",
    "    # e.g. \"/content/drive/MyDrive/assignment\"\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/assignment\"\n",
    "    %cd $ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3twXxeDrr8u"
   },
   "source": [
    "## Install dependencies (Only for Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89953,
     "status": "ok",
     "timestamp": 1680028980620,
     "user": {
      "displayName": "Pablo Pastore",
      "userId": "13868799798828179067"
     },
     "user_tz": 180
    },
    "id": "nxv7tDMorr8y",
    "outputId": "a41ca9a0-8596-4f83-cff1-58ff94a28860"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # This will make sure you have installed all the proper dependencies\n",
    "    # Instal dependencies\n",
    "    !pip install -r requirements.txt\n",
    "    # We can access to GPUs in Colab, so install GPU version of tensorflow\n",
    "    !pip install tensorflow-gpu==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_MAC:\n",
    "    # To access the GPU in Mx Mac, we need to install the Metal Plugin\n",
    "    !pip install tensorflow-macos==2.5.0\n",
    "    !pip install tensorflow-metal==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_k9xfyOXSuH"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This is a multi-class classification task: we aim to predict the correct category of a product from BestBuy.com based on its textual and image data. The raw dataset consists of detailed product information stored in JSON files. Our goal is to classify products into one of the predefined categories using both textual and visual features.\n",
    "\n",
    "The dataset contains two raw JSON files that you can use to understand the data:\n",
    "- `products.json`: Contains information about the products such as name, description, price, etc.\n",
    "- `categories.json`: Contains hierarchical information about the product categories.\n",
    "\n",
    "In this project, we'll provide you with a preprocessed dataset and a zip file containing the images. Place the images in the `data/images` folder in the same directory as the notebook.\n",
    "\n",
    "- Use the file `processed_products_with_images.csv` and place it in the `data` folder.\n",
    "- [Download](https://drive.google.com/file/d/14s2aDNTEWse86cWyLhvVIKmob6EbQrm_/view?usp=sharing) and unzip the file `images.zip` and place the images in the `data/images` folder.\n",
    "\n",
    "The task is divided into several steps:\n",
    "1. **Exploratory Data Analysis (EDA)** on the product dataset.\n",
    "2. **Preprocessing** the dataset to prepare it for model training.\n",
    "3. **Generating embeddings** from both product images and textual descriptions.\n",
    "4. **Training and evaluating machine learning models** using these embeddings to predict the product category.\n",
    "\n",
    "The project will focus on leveraging both text and image data to improve the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6562,
     "status": "ok",
     "timestamp": 1680033789818,
     "user": {
      "displayName": "Pablo Pastore",
      "userId": "13868799798828179067"
     },
     "user_tz": 180
    },
    "id": "nhrF6OAAXSuI"
   },
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from src.utils import ImageDownloader\n",
    "from src.vision_embeddings_tf import get_embeddings_df\n",
    "from src.nlp_models import HuggingFaceEmbeddings\n",
    "from src.nlp_models import GPT\n",
    "from src.utils import preprocess_data, train_test_split_and_feature_extraction\n",
    "import os\n",
    "\n",
    "from src.classifiers_classic_ml import train_and_evaluate_model as train_classic\n",
    "\n",
    "from src.classifiers_mlp import MultimodalDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src.classifiers_mlp import train_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqPF-5jcXSuI"
   },
   "source": [
    "### 1.1 Getting the data\n",
    "\n",
    "To access the data for this project, you only need the file `processed_products_with_images.csv`, and the images from [this link](https://drive.google.com/file/d/14s2aDNTEWse86cWyLhvVIKmob6EbQrm_/view?usp=sharing). You can place the file in the `data` directory and the images in the `data/images` directory. The images were downloaded using the `ImageDownloader` class provided in the `utils.py` file in a 224 x 224 pixel format, but if you want to download the images yourself in a higher resolution, you can use the `ImageDownloader` class to do so. using the code below:\n",
    "\n",
    "```python\n",
    "# Load the data:\n",
    "CSV_PATH = 'data/processed_products.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Download the images and add the image paths to the dataframe:\n",
    "DIR='data/images/'\n",
    "SHAPE=(224, 224)\n",
    "OVERWRITE=False\n",
    "OUTPUT_CSV='data/processed_products_with_images.csv'\n",
    "\n",
    "# Instantiate the ImageDownloader class\n",
    "image_downloader = ImageDownloader(image_dir=DIR, image_size=SHAPE, overwrite=OVERWRITE)\n",
    "\n",
    "# Download images and get the updated DataFrame\n",
    "updated_df = image_downloader.download_images(df)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "updated_df.to_csv(CSV_PATH, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlO06ncGXSuJ"
   },
   "source": [
    "#### 1.1. Simple EDA\n",
    "\n",
    "- Read the data and display the first few rows.\n",
    "- Display some images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the data\n",
    "df = pd.read_csv('data/processed_products_with_images.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 6 random images from the dataset\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        # Get a random index\n",
    "        idx = df.sample().index[0]\n",
    "        # Read the image\n",
    "        img = mpimg.imread(df.loc[idx, 'image_path'])\n",
    "        # Display the image\n",
    "        ax[i, j].imshow(img)\n",
    "        # Set the title\n",
    "        ax[i, j].set_title('\\n'.join(df.loc[idx, 'name'].split()[:3]))\n",
    "        ax[i, j].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Embedding Generation\n",
    "\n",
    "We have two types of data: textual and image data. Unfortunately, these data types are high dimensional and extremely different from each other. To make them compatible for a machine learning model, and homogeneous, we will generate embeddings for both the textual and image data.\n",
    "\n",
    "As you remember from the lectures on embeddings, they are a way to represent high-dimensional data in a lower-dimensional space, some sort of feature extraction. In this project, we will use pre-trained models to generate these embeddings.\n",
    "\n",
    "The first step is to generate embeddings for both the textual and image data. We will use pre-trained models to generate these embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTiXjhZxXSuK"
   },
   "source": [
    "#### 2.1. Image Embeddings\n",
    "\n",
    "We will use a pre-trained model to generate image embeddings. The model will take an image as input and output a vector of fixed size that represents the image. We will use the `get_embeddings_df` function to extract embeddings from images using a backbone model using `Tensorflow` and `Transformers`.\n",
    "\n",
    "use `get_embeddings_df` function to extract embeddings from images using a backbone model.\n",
    "\n",
    "```python\n",
    "get_embeddings_df(\n",
    "    batch_size=32,\n",
    "    path='data/images',\n",
    "    output_dir='Embeddings',\n",
    "    backbone='vit_base'\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "List of possible backbones you can implement:\n",
    "\n",
    "* resnet50\n",
    "* resnet101\n",
    "* densenet121\n",
    "* densenet169\n",
    "* inception_v3\n",
    "* convnextv2_tiny\n",
    "* convnextv2_base\n",
    "* convnextv2_large\n",
    "* swin_tiny\n",
    "* swin_small\n",
    "* swin_base\n",
    "* vit_base\n",
    "* vit_large\n",
    "* clip_base\n",
    "* clip_large\n",
    "\n",
    "**You should at least implement [tensorflow ConvNextV2](https://huggingface.co/docs/transformers/en/model_doc/convnextv2#transformers.TFConvNextV2Model) from Hugging Face and `ResNet50` from [tensorflow.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications).**\n",
    "\n",
    "\n",
    "* Output: The function will create a file `Embeddings_{model_name}.csv` containing the image name and the embeddings for each image in the dataset in the directory `Embeddings`. The embeddings are represented as n columns, where n is the size of the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'data/images'\n",
    "dataset = ''\n",
    "backbone = 'resnet50'  # Use ResNet50 which works without TF transformers\n",
    "out_dir = 'Embeddings_test'\n",
    "\n",
    "# Use ResNet50 only to avoid Keras compatibility issues\n",
    "print(\"Using ResNet50 backbone to avoid transformer compatibility issues...\")\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Text Embeddings\n",
    "\n",
    "For text embeddings, we will use a pre-trained model to generate embeddings for the product descriptions. The model will take a text input and output a vector of fixed size that represents the text. We will use the `HuggingFaceEmbeddings` class to extract embeddings from text using a pre-trained transformer model from the Hugging Face library.\n",
    "\n",
    "* Optional: You can use the `GPT` class to generate embeddings for the product descriptions using a model avaialble in the OpenAI API.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 HuggingFaceEmbeddings\n",
    "\n",
    "The `HuggingFaceEmbeddings` class is used to generate embeddings for text data using pre-trained transformer models from the Hugging Face library. The class takes the following parameters:\n",
    "\n",
    "- `model_name`: The name of the pre-trained transformer model to use.\n",
    "- `device`: The device to use for processing the data ('cpu', 'cuda', 'mps').\n",
    "- `path`: The path to the CSV file containing the text data.\n",
    "\n",
    "The class has the method `get_embedding_df` that generates embeddings for a given text column in a CSV file and saves the embeddings to a new CSV file. The method takes the following parameters:\n",
    "\n",
    "- `column`: The name of the text column in the CSV file.\n",
    "- `directory`: The directory to save the embeddings file.\n",
    "- `file`: The name of the embeddings file to save.\n",
    "\n",
    "```python\n",
    "# Usage example for BERT:\n",
    "device = 'mps'\n",
    "model = HuggingFaceEmbeddings(model_name='bert-base-uncased', device=device)\n",
    "\n",
    "model.path = 'data/processed_products_with_images.csv'\n",
    "column = 'description'\n",
    "directory = 'Embeddings/'\n",
    "file = 'text_embeddings_bert.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)\n",
    "```\n",
    "\n",
    "* Output: The function will create a file with the defined name `file` (e.g., `text_embeddings_bert.csv`) containing the product metadata and the embeddings for each text description in the dataset. The dataset will be stored in the directory `directory` (e.g., `Embeddings/`). The embeddings are represented as a list in a **single column** with the name `embeddings`. **Don't convert the text embeddings into multiple columns in the dataframe, leave it as a list, since the conversion will be done in the next step.**\n",
    "\n",
    "**The class should work at least for `sentence-transformers/all-MiniLM-L6-v2`, but optionally, you can implement it for other models as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example for all-MiniLM-L6-v2:\n",
    "device = 'cpu' # Set to 'cuda' if you have a GPU, 'cpu' otherwise\n",
    "model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "model.path = 'data/processed_products_with_images.csv' # This is the output of the previous step\n",
    "column = 'description'\n",
    "directory = 'Embeddings/'\n",
    "file = 'text_embeddings_minilm.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 GPT **Optional**\n",
    "\n",
    "The `GPT` class is used to generate embeddings for text data using the OpenAI API. \n",
    "\n",
    "\n",
    "* **Backbone:** Select an embedding model from the list of possible [embedding models](https://platform.openai.com/docs/models/embeddings)\n",
    "\n",
    "Ussage example:\n",
    "\n",
    "```python\n",
    "# Choose your model from the list of models:\n",
    "model = GPT(embedding_model='text-embedding-3-small')\n",
    "\n",
    "model.path = 'data/processed_products_with_images.csv'\n",
    "column = 'description'\n",
    "directory = 'Embeddings/'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use OpenAI's API to get the embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Merge Embeddings\n",
    "\n",
    "Once you have the embeddings, you need to merge them into a single dataset. You can use the `preprocess_data` function to merge the embeddings and preprocess the dataset for training.\n",
    "\n",
    "The function needs the following parameters:\n",
    "\n",
    "- `text_data`: The path to the CSV file containing the text embeddings.\n",
    "- `image_data`: The path to the CSV file containing the image embeddings.\n",
    "- `text_id`: The name of the column containing the text IDs.\n",
    "- `image_id`: The name of the column containing the image IDs.\n",
    "- `embeddings_col`: The name of the column containing the embeddings.\n",
    "\n",
    "```python\n",
    "preprocess_data(text_data, image_data, text_id=\"image_id\", image_id=\"ImageName\", embeddings_col = 'embeddings')\n",
    "```\n",
    "\n",
    "* Output: The function will create a dataframe with the embeddings from both the text and image data. The dataframe will contain the text and image IDs, the product category, as well as the metadata for the product, and `n` + `m` new extra columns with the embeddings, where `n` is the size of the text embeddings and `m` is the size of the image embeddings. The new embedding columns will be named `text_0`, `text_1`, ..., `text_n`, `image_0`, `image_1`, ..., `image_m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the embeddings\n",
    "PATH = 'Embeddings/'\n",
    "text_path = 'text_embeddings_minilm.csv'\n",
    "images_path = 'Embeddings_convnextv2_tiny.csv'\n",
    "\n",
    "# Read unimodal data\n",
    "text = pd.read_csv(os.path.join(PATH, text_path))\n",
    "\n",
    "images = pd.read_csv(os.path.join(PATH, images_path))\n",
    "\n",
    "# Merge and preprocess the datasets\n",
    "df = preprocess_data(text, images, \"image_path\", \"ImageName\")\n",
    "\n",
    "df.to_csv(os.path.join(PATH, 'embeddings_minilm.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Models Training\n",
    "\n",
    "We'll be using 2 approaches to train the model:\n",
    "\n",
    "1. A classical machine learning model using the embeddings generated from the text and image data.\n",
    "2. A deep learning model using the embeddings generated from the text and image data to train a neural network.\n",
    "\n",
    "In order to do a fair comparison between the two approaches, we will use the same train/test split for both models. We will use 70% of the data for training and 30% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Preparation\n",
    "\n",
    "The first step is to prepare the dataset for training. We will use the `train_test_split` function to split function from the `sklearn` library to split the dataset into training and testing sets.\n",
    "\n",
    "Then we will define the image embedding columns and the text embedding columns to be used in the model.\n",
    "\n",
    "You can use those columns to train 3 models per approach:\n",
    "* A model using only the image embeddings.\n",
    "* A model using only the text embeddings.\n",
    "* A model using both the image and text embeddings, or multimodal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data:\n",
    "PATH = 'Embeddings/'\n",
    "df = pd.read_csv(os.path.join(PATH, 'embeddings_minilm.csv'))\n",
    "\n",
    "# Split the data into train and test sets and extract the features\n",
    "train_df, test_df, text_columns, image_columns, label_columns = train_test_split_and_feature_extraction(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5ag5bjyXSuL"
   },
   "source": [
    "### 3.2. Classical ML model Training\n",
    "\n",
    "After generating the embeddings, we will train a machine learning model to predict the product category based on the embeddings. We will use the `train_and_evaluate_model` (`train_classic` in this code) function to train a classic machine learning model using the embeddings.\n",
    "\n",
    "**You should at least implement the following models: `RandomForest`, and `LogisticRegression`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Labels\n",
    "y_train = train_df[label_columns].values\n",
    "y_test = test_df[label_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced reload with comprehensive cache clearing\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import inspect\n",
    "\n",
    "#Remove __pycache__ directories to clear bytecode cache\n",
    "cache_dirs = []\n",
    "for root, dirs, files in os.walk('src'):\n",
    "    for d in dirs:\n",
    "        if d == '__pycache__':\n",
    "            cache_dirs.append(os.path.join(root, d))\n",
    "\n",
    "for cache_dir in cache_dirs:\n",
    "    try:\n",
    "        shutil.rmtree(cache_dir)\n",
    "        print(f\"‚úÖ Removed cache directory: {cache_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not remove {cache_dir}: {e}\")\n",
    "\n",
    "#Remove module from sys.modules to force fresh import\n",
    "modules_to_reload = [\n",
    "    'src.classifiers_classic_ml',\n",
    "    'src.utils',\n",
    "    'src.nlp_models',\n",
    "    'src.vision_embeddings_tf',\n",
    "    'src.classifiers_mlp'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "        print(f\"‚úÖ Removed {module_name} from sys.modules\")\n",
    "\n",
    "#Fresh import and reload\n",
    "import src.classifiers_classic_ml\n",
    "importlib.reload(src.classifiers_classic_ml)\n",
    "from src.classifiers_classic_ml import train_and_evaluate_model as train_classic\n",
    "\n",
    "# Verify the function has the latest code by checking signature\n",
    "try:\n",
    "    func_source = inspect.getsource(train_classic)\n",
    "    if \"y_train_flat\" in func_source and \"y_test_flat\" in func_source:\n",
    "        print(\"‚úÖ Function contains label flattening fixes!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Function may not have latest fixes\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not inspect function source: {e}\")\n",
    "\n",
    "print(\"üîÑ Enhanced module reload completed with cache clearing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Train and Evaluate Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text columns to form the feature set\n",
    "X_train = train_df[text_columns].values\n",
    "X_test = test_df[text_columns].values\n",
    "\n",
    "# Train and evaluate models\n",
    "trained_models = train_classic(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Train and Evaluate Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image columns to form the feature set\n",
    "X_train = train_df[image_columns].values\n",
    "X_test = test_df[image_columns].values\n",
    "\n",
    "# Train and evaluate models\n",
    "trained_models = train_classic(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Train and Evaluate Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text and image columns to form the feature set\n",
    "X_train = train_df[text_columns + image_columns].values\n",
    "X_test = test_df[text_columns + image_columns].values\n",
    "\n",
    "# Train and evaluate models\n",
    "trained_models = train_classic(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddODkQqKXSuM"
   },
   "source": [
    "### 3.3 Multi-layer Perceptron\n",
    "\n",
    "*Now it's time to train our first Neural Network model. For simplicity, we are going to try using an MLP model.*\n",
    "\n",
    "*A Multi-layer Perceptron (MLP) is a simple neural network consisting of multiple layers of nodes connected by weighted edges. The input and output layers have one node per feature, and one node per target class respectively, while the intermediate layers have an arbitrary number of nodes.*\n",
    "\n",
    "In this case you'll train 3 models:\n",
    "* A model using only the image embeddings.\n",
    "* A model using only the text embeddings.\n",
    "* A model using both the image and text embeddings, or multimodal model.\n",
    "\n",
    "In order to do that, you can use the `train_mlp` function to train a neural network model using the embeddings. Is important that you create a correct setup in `src/classifiers_mlp.py` to train the model.\n",
    "\n",
    "Once you train each model, it will generate a classification report and a confusion matrix to evaluate the model's performance. It will also generate a folder `results` with the files `results/multimodal_results.csv`, `results/image_results.csv`, or `results/text_results.csv` containing the results of your model on the test set. Those files are necessary for the final evaluation, so don't remove, rename or change them.\n",
    "\n",
    "**You must achieve at least 85% accuracy and 80% F1-score for the multimodal model, an accuracy of at least 85% and an F1-score of at least 80% for the text model, and an accuracy of at least 75% and an F1-score of at least 70% for the image model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Perpare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253679,
     "status": "ok",
     "timestamp": 1680034364477,
     "user": {
      "displayName": "Pablo Pastore",
      "userId": "13868799798828179067"
     },
     "user_tz": 180
    },
    "id": "Zf2uzXf_XSuM",
    "outputId": "ec3837de-4700-4820-9cf0-8e64c8c4ac03"
   },
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder and fit it only on the training data\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df[label_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Train and Evaluate the image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and ensure the encoder is passed to both train and test sets\n",
    "train_dataset = MultimodalDataset(train_df, text_cols=None, image_cols=image_columns, label_col=label_columns, encoder=label_encoder)\n",
    "test_dataset = MultimodalDataset(test_df, text_cols=None, image_cols=image_columns, label_col=label_columns, encoder=label_encoder)\n",
    "\n",
    "text_input_size = None\n",
    "image_input_size = len(image_columns)\n",
    "output_size = label_encoder.classes_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Image Model:\")\n",
    "train_mlp(train_dataset, test_dataset,\n",
    "            text_input_size=text_input_size, \n",
    "            image_input_size=image_input_size, \n",
    "            output_size=output_size, \n",
    "            num_epochs=50,\n",
    "            report=True, \n",
    "            lr=0.001, \n",
    "            set_weights=True, \n",
    "            adam=True, \n",
    "            p=0.2,  \n",
    "            seed=42, \n",
    "            patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. Train and Evaluate the text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and ensure the encoder is passed to both train and test sets\n",
    "train_dataset = MultimodalDataset(train_df, text_cols=text_columns, image_cols=None, label_col=label_columns, encoder=label_encoder)\n",
    "test_dataset = MultimodalDataset(test_df, text_cols=text_columns, image_cols=None, label_col=label_columns, encoder=label_encoder)\n",
    "\n",
    "text_input_size = len(text_columns)\n",
    "image_input_size = None\n",
    "output_size = label_encoder.classes_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the updated classifier modules to pick up the bug fix\n",
    "import importlib\n",
    "import src.classifiers_mlp\n",
    "importlib.reload(src.classifiers_mlp)\n",
    "from src.classifiers_mlp import MultimodalDataset, train_mlp\n",
    "print(\"‚úÖ Modules reloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Text Model:\")\n",
    "train_mlp(train_dataset, test_dataset,\n",
    "            text_input_size=text_input_size, \n",
    "            image_input_size=image_input_size, \n",
    "            output_size=output_size, \n",
    "            num_epochs=30,\n",
    "            report=True, \n",
    "            lr=0.003, \n",
    "            set_weights=True, \n",
    "            adam=True, \n",
    "            p=0.5,  \n",
    "            seed=42, \n",
    "            patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4. Train and Evaluate the fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and ensure the encoder is passed to both train and test sets\n",
    "train_dataset = MultimodalDataset(train_df, text_cols=text_columns, image_cols=image_columns, label_col=label_columns, encoder=label_encoder)\n",
    "test_dataset = MultimodalDataset(test_df, text_cols=text_columns, image_cols=image_columns, label_col=label_columns, encoder=label_encoder)\n",
    "\n",
    "text_input_size = len(text_columns)\n",
    "image_input_size = len(image_columns)\n",
    "output_size = label_encoder.classes_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Early Fusion Model:\")\n",
    "train_mlp(train_dataset, test_dataset,\n",
    "            text_input_size=text_input_size, \n",
    "            image_input_size=image_input_size, \n",
    "            output_size=output_size, \n",
    "            num_epochs=10,\n",
    "            report=True, \n",
    "            lr=0.001, \n",
    "            set_weights=True, \n",
    "            adam=True, \n",
    "            p=0.2,  \n",
    "            seed=42, \n",
    "            patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNisyJIuXSuM"
   },
   "source": [
    "Take into account that the model trained will be evaluated in multiple classes. It's important to use the right loss function and metrics for this task. \n",
    "\n",
    "Check the perfermance of the model in each class. Is the model overfitting? Underfitting? What can you do to improve the model?\n",
    "\n",
    "Aditionally, the fusion model is performing better than the individual models? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY2lgAH-XSuN"
   },
   "source": [
    "## 7. Optional. Make your own model\n",
    "\n",
    "The models we've trained before are just a limited set over the universe of stuff you can use. \n",
    "\n",
    "You still have a lot of things to experiment with to increase accuracy, some ideas are:\n",
    "\n",
    "1. Use other pre-trained models for text and image embeddings.\n",
    "2. Use closed-source models like OpenAI's models from the API.\n",
    "2. Try adding data augmentation or any other regularization algorithms.\n",
    "4. Check [KerasTuner](https://keras.io/api/keras_tuner/) documentation about how to efficiently test a lot of different architectures and parameters.\n",
    "5. Is there a trade-off between the different models in terms of performance and computational cost? How would you decide which model to use in a production environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4YozyqCvqz6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
